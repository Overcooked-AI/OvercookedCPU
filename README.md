# MAPPO Training for Overcooked-AI ğŸ§‘â€ğŸ³ğŸ¤–

Train Multi-Agent Proximal Policy Optimization (MAPPO) agents on CPU using the Overcooked-AI environment with advanced features including curriculum learning, adaptive learning rates, and comprehensive behavioral analysis.

## âœ¨ Current Features

### ğŸ¯ Core Training
- âœ… **MAPPO Implementation** - Shared policy with centralized value function
- âœ… **CPU-Optimized** - Vectorized observations, no rendering required
- âœ… **Padded Observations** - Transfer learning across different map sizes
- âœ… **Reward Shaping** - Optional potential-based shaped rewards
- âœ… **Multiple Layouts** - 5 built-in + 5 custom playground layouts

### ğŸ“Š Advanced Training Features
- âœ… **Curriculum Learning** - Progressive difficulty scheduling
  - Progressive: Easy â†’ Medium â†’ Hard
  - Mixed: Random difficulty sampling
  - Reverse: Hard â†’ Easy (robustness training)
- âœ… **Adaptive Learning Rate** - Dynamic LR adjustment
  - Cosine annealing
  - Step decay
  - Plateau detection
- âœ… **Hyperparameter Optimization** - Ray Tune integration
  - ASHA scheduler for early stopping
  - Population-Based Training (PBT)
  - Optuna search algorithm
- âœ… **Multi-Layout Training** - Single policy across multiple maps

### ğŸ”¬ Monitoring & Analysis
- âœ… **Real-Time Monitoring** - TensorBoard integration
  - Core metrics (reward, length, losses)
  - Policy metrics (entropy, KL divergence)
  - Resource metrics (sample/learn time)
- âœ… **Coordination Metrics** - Specialized MARL metrics
  - Task specialization (who carries what)
  - Collision frequency
  - Handoff coordination
  - Action synchronization
  - Spatial coverage & overlap
  - Steps per soup efficiency
- âœ… **Behavioral Analysis** - Comprehensive post-training analysis
  - Position heatmaps
  - Action distribution analysis
  - Task specialization visualization
  - Automated report generation
- âœ… **Live Training Monitor** - Real-time console dashboard

### ğŸ› ï¸ Developer Tools
- âœ… **Checkpoint Management** - Save/load/evaluate models
- âœ… **Custom Callbacks** - Extensible training hooks
- âœ… **Progress Tracking** - Rolling statistics (100 episodes)
- âœ… **Visualization Suite** - Matplotlib/Seaborn plotting
- âœ… **Error Handling** - Robust fallbacks and warnings

## ğŸš€ Future Features

### ğŸ® Gameplay & Interaction
- ğŸ”² **Human-AI Interface** - Web-based interface for human players
  - Real-time gameplay with trained agents
  - Action probability visualization
  - Intent prediction display
- ğŸ”² **Replay System** - Record and playback episodes
  - Video generation from trajectories
  - State-by-state inspection
- ğŸ”² **Multi-Agent Scenarios** - Beyond 2-player
  - 3-4 agent coordination
  - Asymmetric teams

### ğŸ¤– Advanced Algorithms
- ğŸ”² **Value Decomposition** - QMIX, QTRAN implementations
- ğŸ”² **Communication Protocols** - Learned agent communication
  - CommNet, TarMAC architectures
- ğŸ”² **Hierarchical RL** - High-level goal + low-level actions
- ğŸ”² **Meta-Learning** - Few-shot adaptation to new layouts
- ğŸ”² **Inverse RL** - Learn from human demonstrations

### ğŸ“ˆ Training Enhancements
- ğŸ”² **Prioritized Experience Replay** - Better sample efficiency
- ğŸ”² **Hindsight Experience Replay** - Learn from failures
- ğŸ”² **Curiosity-Driven Exploration** - Intrinsic motivation
- ğŸ”² **Self-Play Evolution** - Train against past selves
- ğŸ”² **Opponent Modeling** - Predict partner behavior
- ğŸ”² **Multi-Task Learning** - Train on multiple objectives simultaneously

### ğŸ§  Intelligence Features
- ğŸ”² **Theory of Mind** - Model partner's beliefs
- ğŸ”² **Emergent Language** - Agents develop communication
- ğŸ”² **Concept Learning** - Abstract task representations
- ğŸ”² **Few-Shot Generalization** - Quick adaptation to new partners

### ğŸ” Analysis & Interpretability
- ğŸ”² **Attention Visualization** - What agents focus on
- ğŸ”² **Counterfactual Analysis** - "What if" scenarios
- ğŸ”² **Skill Discovery** - Automatic primitive identification
- ğŸ”² **Failure Mode Classification** - Automatic bug detection
- ğŸ”² **Coordination Score** - Quantitative metrics for teamwork

### ğŸŒ Infrastructure
- ğŸ”² **Distributed Training** - Multi-node Ray cluster support
- ğŸ”² **GPU Acceleration** - Optional GPU training
- ğŸ”² **Cloud Integration** - AWS/GCP deployment scripts
- ğŸ”² **Model Serving** - REST API for inference
- ğŸ”² **MLflow Integration** - Experiment tracking
- ğŸ”² **Weights & Biases** - Advanced logging

### ğŸ¯ Domain Extensions
- ğŸ”² **Custom Recipes** - More complex cooking mechanics
- ğŸ”² **Dynamic Obstacles** - Moving hazards
- ğŸ”² **Stochastic Events** - Random failures, delays
- ğŸ”² **Resource Constraints** - Limited ingredients, time pressure
- ğŸ”² **Procedural Generation** - Infinite layout variations

### ğŸ¤ Human-AI Coordination
- ğŸ”² **Adaptation to Human Style** - Real-time partner modeling
- ğŸ”² **Legibility** - Making actions interpretable to humans
- ğŸ”² **Assistive Agents** - Predict and fill human intentions
- ğŸ”² **Human Data Collection** - Integrated annotation tools
- ğŸ”² **Preference Learning** - Learn from human feedback

## ğŸ“ Project Structure

```
your_project/
â”œâ”€â”€ train_mappo.py              # Basic MAPPO training
â”œâ”€â”€ adaptive_trainer.py         # Curriculum + adaptive LR
â”œâ”€â”€ tune_hyperparams.py         # Hyperparameter optimization
â”œâ”€â”€ analyze_behavior.py         # Behavioral analysis tools
â”œâ”€â”€ training_monitor.py         # TensorBoard monitoring
â”œâ”€â”€ coordination_metrics.py     # Specialized coordination metrics
â”œâ”€â”€ overcooked_mappo_env.py     # RLlib environment wrapper
â”œâ”€â”€ custom_layout.py            # Custom playground layouts
â”œâ”€â”€ config.py                   # Configuration parameters
â”œâ”€â”€ utils.py                    # Utility functions
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ quickstart.sh              # Quick start script
â””â”€â”€ README.md                  # This file
```

## ğŸš€ Quick Start

### Basic Training
```bash
# Simple training
python train_mappo.py

# With reward shaping
python train_mappo.py --use-phi

# Different layout
python train_mappo.py --layout asymmetric_advantages --iterations 1000
```

### Advanced Training
```bash
# Curriculum learning + adaptive LR
python adaptive_trainer.py \
    --enable-curriculum \
    --enable-adaptive-lr \
    --curriculum-type progressive \
    --lr-strategy cosine_annealing \
    --iterations 1000

# Hyperparameter optimization
python tune_hyperparams.py \
    --search-type medium \
    --num-samples 20 \
    --scheduler asha
```

### Analysis & Monitoring
```bash
# Live monitoring
python training_monitor.py --results-dir ./results

# TensorBoard
tensorboard --logdir ./results

# Behavioral analysis
python analyze_behavior.py \
    --checkpoint ./results/checkpoint_500 \
    --layout cramped_room \
    --episodes 50
```

## ğŸ® Available Layouts

**Built-in Layouts:**
- `cramped_room` - Small kitchen, tight coordination
- `asymmetric_advantages` - Asymmetric roles
- `coordination_ring` - Circular layout
- `forced_coordination` - Requires tight coordination
- `counter_circuit` - Long counter layout

**Custom Playground Layouts:**
- `playground` - Basic playground (5x5)
- `playground_medium` - Medium-sized kitchen (7x6)
- `playground_large` - Large kitchen (9x6)
- `playground_complex` - Multiple pots and obstacles (11x6)
- `playground_corridor` - Narrow corridor (7x6)

## ğŸ”§ Configuration

### Environment Settings (`config.py`)
```python
ENV_CONFIG = {
    "layout_name": "cramped_room",
    "horizon": 400,
    "use_phi": False,
    "reward_shaping_factor": 1.0,
}
```

### Training Hyperparameters
```python
TRAINING_CONFIG = {
    "num_workers": 4,              # Parallel environments
    "num_envs_per_worker": 4,      # Vectorized envs
    "train_batch_size": 4000,      # Samples per iteration
    "lr": 5e-4,                    # Learning rate
    "gamma": 0.99,                 # Discount factor
    "entropy_coeff": 0.01,         # Exploration bonus
}
```

## ğŸ“Š Monitoring Metrics

### Core Metrics
- **Episode Reward Mean** - Average return per episode
- **Episode Length** - Average steps per episode
- **Policy Loss** - Policy gradient loss
- **Value Loss** - Value function MSE
- **Entropy** - Policy exploration measure
- **KL Divergence** - Policy change magnitude

### Coordination Metrics
- **Soups Delivered** - Task completion count
- **Steps per Soup** - Efficiency measure
- **Collisions per 100 Steps** - Conflict frequency
- **Task Balance** - Work distribution equality
- **Ingredient Gathering Balance** - Role specialization
- **Spatial Overlap** - Shared workspace usage
- **Action Synchronization** - Joint action patterns
- **Complementary Actions** - Move-interact coordination

## ğŸ§  How MAPPO Works

**Multi-Agent PPO (MAPPO)** achieves coordination through:

1. **Shared Policy**: Both agents use identical network weights
   - Reduces sample complexity
   - Enables symmetric coordination
   - Natural for homogeneous agents

2. **Parameter Sharing**: Single network for all agents
   - Faster learning from pooled experiences
   - Better generalization across scenarios

3. **Centralized Training**: Uses full state during learning
   - Critic sees global information
   - Actor only needs local observations

4. **Decentralized Execution**: Agents act independently
   - No communication required at test time
   - Robust to partial observability

### Key Features
- âœ… Vectorized observations via `lossless_state_encoding`
- âœ… Padded to fixed size (10Ã—10) for transfer learning
- âœ… No image rendering (CPU-efficient)
- âœ… Cooperative reward (both agents get same signal)
- âœ… Full game mechanics (pickup, place, cook, deliver)

## ğŸ“ˆ Expected Performance

| Layout | Training Time | Final Reward | Soups/Episode |
|--------|---------------|--------------|---------------|
| cramped_room | 30-60 min | 15-25 | 3-5 |
| asymmetric_advantages | 45-90 min | 20-30 | 4-6 |
| coordination_ring | 60-120 min | 10-20 | 2-4 |
| forced_coordination | 90-180 min | 15-25 | 3-5 |

*(4 workers, 4 envs/worker on modern CPU)*

## ğŸ”¬ Advanced Usage

### Curriculum Learning
```python
# Progressive difficulty
python adaptive_trainer.py \
    --enable-curriculum \
    --curriculum-type progressive

# Reverse curriculum (robustness)
python adaptive_trainer.py \
    --enable-curriculum \
    --curriculum-type reverse
```

### Adaptive Learning Rate
```python
# Cosine annealing
python adaptive_trainer.py \
    --enable-adaptive-lr \
    --lr-strategy cosine_annealing

# Plateau-based reduction
python adaptive_trainer.py \
    --enable-adaptive-lr \
    --lr-strategy plateau
```

### Hyperparameter Optimization
```python
# Quick search (6 params)
python tune_hyperparams.py --search-type quick --num-samples 10

# Medium search (11 params)
python tune_hyperparams.py --search-type medium --num-samples 20

# Full search (16 params)
python tune_hyperparams.py --search-type full --num-samples 50
```

### Behavioral Analysis
```python
# Generate all visualizations
python analyze_behavior.py \
    --checkpoint ./results/checkpoint_500 \
    --episodes 100 \
    --save-dir ./analysis

# Output:
# - Position heatmaps
# - Action distribution plots
# - Task specialization charts
# - Comprehensive text report
```

## ğŸ› Troubleshooting

**Ray initialization errors:**
```bash
ray stop  # Stop existing processes
python train_mappo.py
```

**Out of memory:**
- Reduce `num_workers` or `num_envs_per_worker`
- Reduce `train_batch_size`
- Close other applications

**Slow training:**
- Increase `num_workers` (up to CPU cores)
- Increase `num_envs_per_worker` (test 2-8)
- Reduce observation padding size if using small maps only

**Agent not learning:**
- Enable reward shaping: `--use-phi`
- Try curriculum learning
- Increase training duration
- Check TensorBoard: `tensorboard --logdir ./results`

**TensorBoard not showing data:**
```bash
pip install tensorboard
# Check log directory exists
ls results/*/tensorboard/
```

## ğŸ“š Citations

If you use this code, please cite:

```bibtex
@article{carroll2019utility,
  title={On the utility of learning about humans for human-ai coordination},
  author={Carroll, Micah and Shah, Rohin and Ho, Mark K and Griffiths, Tom and Seshia, Sanjit and Abbeel, Pieter and Dragan, Anca},
  journal={NeurIPS},
  year={2019}
}

@article{yu2022surprising,
  title={The surprising effectiveness of ppo in cooperative multi-agent games},
  author={Yu, Chao and Velu, Akash and Vinitsky, Eugene and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  journal={NeurIPS},
  year={2022}
}
```

## ğŸ¤ Contributing

Contributions welcome! Key areas:
- Additional reward shaping functions
- More efficient state encodings
- New coordination metrics
- Curriculum learning strategies
- Human-AI interaction experiments

## ğŸ“„ License

This project uses the Overcooked-AI environment (MIT License).

## ğŸ”— Resources

- [Overcooked-AI GitHub](https://github.com/HumanCompatibleAI/overcooked_ai)
- [Ray RLlib Docs](https://docs.ray.io/en/latest/rllib/index.html)
- [MAPPO Paper](https://arxiv.org/abs/2103.01955)
- [Original Overcooked Paper](https://arxiv.org/abs/1910.05789)

---

**Happy Training! ğŸš€**